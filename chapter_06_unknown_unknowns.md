# Chapter 6: The Art of Identifying Unknown Unknowns

On February 12, 2002, U.S. Secretary of Defense Donald Rumsfeld gave a press briefing that would become infamous. Asked about the lack of evidence linking Iraq to weapons of mass destruction, he replied:

"There are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns—the ones we don't know we don't know. And if one looks throughout the history of our country and other free countries, it is the latter category that tends to be the difficult ones."¹

The press mocked Rumsfeld's apparent word salad, but he was articulating one of the deepest challenges in decision-making. The things we don't know we don't know—the unknown unknowns—are often what destroy companies, end careers, and reshape history.

Consider some of history's most consequential unknown unknowns:

- Before 1492, Europeans didn't know they didn't know about the Americas
- Before 1928, doctors didn't know they didn't know about antibiotics
- Before 2007, rating agencies didn't know they didn't know about correlation in mortgage defaults
- Before 2019, pandemic planners didn't know they didn't know about global supply chain fragility
- Before 2022, European leaders didn't know they didn't know about their energy vulnerability

In each case, it wasn't just that people lacked information—they didn't even know what information to look for. They were blind to their blindness, ignorant of their ignorance. The possibility space they considered didn't include the thing that would matter most.

This chapter explores the art and science of identifying unknown unknowns before they blindside us. While we can never eliminate surprise entirely, we can expand our awareness, probe our blind spots, and build resilience to the unimaginable.

## The Rumsfeld Matrix

Rumsfeld's framework, while linguistically awkward, provides a powerful tool for categorizing knowledge:

**Known Knowns**: Things we know we know
- Our skills and expertise
- Documented facts and data
- Established laws and patterns
- Historical precedents

**Known Unknowns**: Things we know we don't know
- Identified risks and uncertainties
- Recognized knowledge gaps
- Pending decisions and outcomes
- Questions we're actively investigating

**Unknown Unknowns**: Things we don't know we don't know
- Blind spots in our mental models
- Possibilities outside our paradigms
- Emergent phenomena
- Black swan events

There's actually a fourth category Rumsfeld didn't mention:

**Unknown Knowns**: Things we don't know we know
- Tacit knowledge and intuitions
- Unconscious competencies
- Cultural assumptions
- Suppressed or forgotten knowledge

This 2×2 matrix maps the landscape of knowledge and ignorance:

|              | Known to Self | Unknown to Self |
|--------------|---------------|-----------------|
| **Known to Others** | Known Knowns | Unknown Knowns |
| **Unknown to Others** | Known Unknowns | Unknown Unknowns |

The art of identifying unknown unknowns involves moving possibilities from the bottom-right quadrant to the bottom-left—transforming unknown unknowns into known unknowns that we can then investigate, prepare for, or at least acknowledge.

## Why Unknown Unknowns Matter

In stable, predictable environments, unknown unknowns are rare and manageable. But in complex, rapidly changing systems, they become the dominant source of risk and opportunity. Consider:

**Technology**: Every major technological disruption started as an unknown unknown. Kodak's engineers invented the digital camera but couldn't imagine a world without film. Blockbuster's executives knew about streaming but couldn't imagine people preferring it to stores. The music industry knew about file sharing but couldn't imagine Napster.

**Finance**: The 2008 crisis wasn't caused by known risks going bad but by unknown unknowns materializing. No one's models included the possibility that AAA-rated securities could become worthless, that liquidity could disappear overnight, or that correlations could go to 1.0 in a crisis.

**Geopolitics**: The biggest geopolitical surprises come from unknown unknowns. The fall of the Berlin Wall, the Arab Spring, Brexit, the January 6th Capitol riot—none were in mainstream scenario planning. They weren't considered possible until they happened.

**Personal Life**: The most significant life events are often unknown unknowns. You don't know you'll meet your future spouse at that coffee shop. You don't know that hobby will become your career. You don't know that side project will change everything.

The philosopher Nassim Taleb calls these "Black Swans"—events that are rare, have extreme impact, and are retrospectively predictable². They're unknown unknowns that reshape the world.

## The Cognitive Challenge

Human cognition is poorly equipped to handle unknown unknowns. Our brains evolved to pattern-match against experience, but unknown unknowns by definition lie outside experience. Several cognitive limitations make us blind to unknown unknowns:

**Availability Bias**: We overweight easily recalled examples. If we can't recall something, we assume it's impossible.

**Confirmation Bias**: We seek information confirming our worldview. Unknown unknowns usually contradict our worldview.

**Narrative Fallacy**: We create coherent stories about the world. Unknown unknowns would make our stories incoherent.

**Expertise Paradox**: The more expert we become, the stronger our paradigms and the blinder we become to paradigm-breaking possibilities.

**Social Proof**: We look to others for validation. But if something is an unknown unknown to us, it's probably an unknown unknown to our peers too.

The result is what psychologist Daniel Kahneman calls WYSIATI—"What You See Is All There Is."³ We act as if our mental model contains all relevant possibilities. We literally can't imagine what we can't imagine.

## Techniques for Surfacing Unknown Unknowns

While we can't identify all unknown unknowns, we can systematically probe for them:

### Pre-Mortem Analysis

Gary Klein's pre-mortem technique, which we've mentioned before, is particularly powerful for surfacing unknown unknowns⁴. The process:

1. Imagine it's one year from now and your project has failed catastrophically
2. Each team member independently writes a story of that failure
3. Share and discuss the failure modes
4. Identify which failures weren't in your risk register

The magic is that imagining failure frees the mind from conventional thinking. People surface concerns they didn't know they had, possibilities they hadn't consciously considered.

A software company used pre-mortems before a major launch. One engineer imagined failure due to "a law passed requiring data localization in Europe." This wasn't on anyone's radar, but GDPR was passed 18 months later, validating the concern. The company was prepared because a pre-mortem surfaced this unknown unknown.

### Red Team Thinking

Red teaming, borrowed from military planning, involves creating a group whose job is to challenge assumptions and find vulnerabilities. But effective red teaming goes beyond devil's advocacy to actively search for unknown unknowns:

- **Assumption Mapping**: List every assumption underlying your plan. For each, ask: "What if this is wrong?"
- **Alternative Narratives**: Create multiple competing explanations for the same facts
- **Perspective Taking**: How would someone from a different culture, industry, or worldview see this?
- **Paradigm Breaking**: What would have to be true for our entire framework to be wrong?

The CIA created a Red Cell after 9/11 specifically to think like terrorists and imagine possibilities the intelligence community wasn't considering⁵. Their reports, deliberately provocative, have surfaced numerous unknown unknowns that became known unknowns worth monitoring.

### The "What Would Have to Be True" Framework

This technique, popularized by Roger Martin, flips conventional thinking⁶. Instead of asking "What do we believe?" ask "What would have to be true for X to happen?"

Examples:
- What would have to be true for our biggest competitor to destroy us?
- What would have to be true for our industry to become obsolete?
- What would have to be true for this decision to be remembered as a historic mistake?

This reframing escapes the confirmation bias that normally constrains thinking. You're not asserting these things are true, just exploring what would be required. This often surfaces assumptions you didn't know you were making.

### Scenario Planning with Wild Cards

Traditional scenario planning explores known uncertainties (oil prices, interest rates, demographic shifts). But effective scenario planning also includes "wild cards"—low-probability, high-impact events that break the scenario framework:

- Technological breakthroughs that change fundamental constraints
- Social movements that reshape values overnight
- Natural disasters that cascade into systemic crises
- Geopolitical events that restructure global order

Shell's scenario planning famously included wild cards that helped them prepare for the 1973 oil crisis and the fall of the Soviet Union⁷. While they didn't predict these events, they had considered "what if" they happened.

### Cross-Domain Learning

Unknown unknowns often come from outside your domain. The retail industry's unknown unknown (e-commerce) came from technology. The taxi industry's unknown unknown (Uber) came from mobile apps. The hotel industry's unknown unknown (Airbnb) came from the sharing economy.

Systematic cross-domain learning can surface these threats and opportunities:

- Study disruptions in other industries
- Attend conferences outside your field
- Read broadly across disciplines
- Cultivate diverse networks
- Hire people with unconventional backgrounds

The key insight: patterns repeat across domains. What's an unknown unknown in your field might be a known known somewhere else.

### Deep Hang-Out Sessions

Psychologist Gary Klein studies how experts surface unknown unknowns through what he calls "deep hang-out sessions"⁸. These are unstructured conversations where experts explore edge cases, anomalies, and "that's funny" moments.

The process:
1. Gather people with deep but diverse expertise
2. Create psychological safety for wild speculation
3. Focus on anomalies and things that don't fit
4. Follow curiosity rather than agenda
5. Document insights without immediate judgment

These sessions work because unknown unknowns often announce themselves through subtle anomalies. The expert who says "that's weird" or "that doesn't make sense" might be detecting an unknown unknown trying to become known.

## Building Robust Strategies

Since we can't identify all unknown unknowns, we need strategies that work even when surprised. This robustness differs from optimization—it's about surviving the unimaginable, not maximizing the expected.

### Antifragility

Nassim Taleb's concept of "antifragility" describes systems that gain from disorder⁹. While fragile systems break under stress and robust systems resist stress, antifragile systems get stronger from stress (up to a point).

Antifragile strategies for unknown unknowns:

**Barbell Strategy**: Combine extreme safety with extreme risk. Put 90% in super-safe investments and 10% in moonshots. You're protected from disaster while positioned for breakthroughs.

**Optionality**: Maintain multiple options that could pay off in different futures. The cost is redundancy; the benefit is adaptation to surprise.

**Small Failures**: Expose yourself to small failures that teach without destroying. Each failure reveals unknown unknowns while you can still recover.

**Redundancy**: Build slack and buffers into systems. Efficiency is fragile; redundancy is robust.

**Decentralization**: Distributed systems fail gracefully. When one part encounters an unknown unknown, others continue functioning.

### Via Negativa

Sometimes the best response to unknown unknowns is subtraction, not addition. Taleb calls this "via negativa"—improving by removing rather than adding¹⁰.

- Remove single points of failure
- Eliminate dependencies you don't understand
- Avoid strategies that require precise predictions
- Stop doing things with unlimited downside
- Reduce complexity that could hide unknown unknowns

This approach acknowledges that we might not know what will help but can know what might hurt.

### The Precautionary Principle

For decisions with potentially catastrophic unknown unknowns, the precautionary principle suggests assuming the worst until proven otherwise. This seems paranoid but is rational when:

- Consequences could be irreversible
- The system is poorly understood
- History offers no good precedents
- Errors compound rather than cancel

Examples include:
- New drug approval (assume harmful until proven safe)
- Genetic engineering (assume ecological damage until proven benign)
- AI development (assume existential risk until proven controlled)

The precautionary principle trades efficiency for survival—appropriate when unknown unknowns could be existential.

## Real Options Thinking

Financial options give you the right but not obligation to take an action. Real options thinking applies this to strategy, creating ways to benefit from unknown unknowns without committing to them.

Types of real options for unknown unknowns:

**Scout Options**: Small investments to learn about new domains. A traditional company hiring a few blockchain developers, not to build products but to understand possibilities.

**Platform Options**: Building capabilities that could serve unknown future needs. Amazon's AWS started as internal infrastructure but became a massive business serving needs Amazon didn't anticipate.

**Switching Options**: Maintaining ability to change direction. Leasing rather than buying, contracting rather than hiring, partnering rather than acquiring.

**Abandonment Options**: Building in graceful exits. Sunset clauses in contracts, modular architectures in systems, kill criteria in projects.

**Growth Options**: Preserving ability to scale if opportunity emerges. Trademark registrations in countries you don't serve, land adjacent to your facilities, relationships with potential partners.

Each option costs something (money, time, complexity) but provides protection or opportunity when unknown unknowns materialize.

## Organizational Approaches

Organizations can structure themselves to better detect and handle unknown unknowns:

### Diverse Teams

Homogeneous teams share blind spots. Diverse teams—across backgrounds, disciplines, cognitive styles, and perspectives—have non-overlapping blind spots. What's an unknown unknown to one member might be obvious to another.

But diversity only helps with specific conditions:
- Psychological safety to voice unconventional ideas
- Processes to surface and integrate diverse perspectives
- Leadership that values discovery over consensus
- Incentives that reward identifying problems, not just solutions

### Exploration Units

Some organizations create dedicated units to explore unknown unknowns:

- Google's X (formerly Google X) explores moonshot technologies
- Amazon's Grand Challenge team investigates existential threats
- The CIA's Red Cell imagines adversary perspectives
- Microsoft Research pursues fundamental questions without immediate application

These units operate outside normal constraints, with different metrics, longer timelines, and permission to fail. They're the organization's sensors for unknown unknowns.

### Learning from Failure

Unknown unknowns often reveal themselves through failure. Organizations that systematically learn from failure build maps of their ignorance:

**Blameless Post-Mortems**: Focus on discovering what happened, not who's at fault. This surfaces unknown unknowns without triggering defensiveness.

**Near-Miss Analysis**: Study close calls, not just failures. Near-misses reveal unknown unknowns that almost materialized.

**Failure Databases**: Document and share failures across the organization. Patterns across failures often point to systematic unknown unknowns.

**Celebration of Discovery**: Reward people who surface unknown unknowns, even if it reveals problems. You want to know what you don't know.

## Personal Practices

Individuals can also develop practices for identifying unknown unknowns:

### Intellectual Humility

The first step is acknowledging the vastness of your ignorance. As physicist Richard Feynman said, "I would rather have questions that can't be answered than answers that can't be questioned."¹¹

Practices for intellectual humility:
- Regularly say "I don't know"
- Seek out people who disagree with you
- Read your old predictions and note what surprised you
- Study domains where you're a complete beginner
- Question your most fundamental assumptions

### Cognitive Diversity

Expose yourself to diverse thinking:
- Read fiction from other cultures
- Study history from multiple perspectives
- Learn from disciplines far from your expertise
- Travel to places that challenge your worldview
- Cultivate friendships across different backgrounds

### Anomaly Attention

Unknown unknowns often announce themselves through anomalies—things that don't fit your mental model. Train yourself to notice and investigate anomalies rather than explaining them away:

- Keep an anomaly journal
- When something seems weird, dig deeper
- Look for patterns across anomalies
- Share anomalies with others for perspective
- Treat confusion as signal, not noise

### Regular Paradigm Challenges

Schedule regular sessions to challenge your paradigms:
- Monthly: Question one professional assumption
- Quarterly: Explore one alternative worldview
- Annually: Identify one thing you were completely wrong about

## The Paradox of Preparation

There's a fundamental paradox in preparing for unknown unknowns: the better prepared you are for specific scenarios, the less prepared you might be for true surprises. This is because:

- Specific preparation creates mental models that blind you to other possibilities
- Resource allocation to known threats reduces resources for unknown ones
- Success with existing approaches reduces willingness to consider alternatives
- Expertise in current paradigms makes paradigm shifts harder to see

The solution isn't to stop preparing but to prepare differently:

**Prepare for categories, not specifics**: Instead of preparing for a pandemic, prepare for "systemic disruptions to normal operations."

**Build capabilities, not just plans**: Develop general problem-solving capacity rather than specific response protocols.

**Maintain strategic reserves**: Keep resources (time, money, attention) uncommitted for unexpected opportunities or threats.

**Practice adaptation**: Regularly do things differently just to maintain flexibility. Change routines, try new approaches, break your own rules.

## Living with Unknown Unknowns

Ultimately, we must accept that unknown unknowns are inherent to existence. We can reduce our blindness but never eliminate it. This acceptance, paradoxically, is empowering. It frees us from the impossible burden of omniscience and lets us focus on what we can do:

- Build resilient rather than optimal systems
- Maintain humility about our knowledge
- Stay curious and open to surprise
- Learn quickly when surprised
- Help others see their blind spots
- Celebrate the joy of discovery

As Donald Rumsfeld's quote suggests, unknown unknowns have shaped history more than any other category of knowledge. They will continue to do so. The question isn't whether we'll be surprised but whether we'll be ready to learn and adapt when we are.

## Practical Exercises

### Exercise 1: Personal Unknown Unknown Audit
1. List 10 major surprises in your life
2. For each, identify why it was surprising
3. Look for patterns—what types of things surprise you?
4. Identify three current blind spots these patterns suggest

### Exercise 2: The Opposite Day
For one decision you're facing:
1. List all your assumptions
2. For each assumption, write its opposite
3. Explore what would have to be true for the opposite to be correct
4. Identify which opposite-worlds you've never considered

### Exercise 3: Cross-Domain Pattern Matching
1. Choose an industry disruption (e.g., Netflix vs. Blockbuster)
2. Map the pattern of disruption
3. Apply that pattern to your industry/life
4. What unknown unknown does this suggest?

### Exercise 4: Pre-Mortem Practice
For your next project:
1. Imagine total failure in one year
2. Write five different failure stories
3. Identify which failures weren't in your planning
4. Add these to your risk register as "newly known unknowns"

### Exercise 5: Anomaly Journal
For one month:
1. Note anything that surprises or confuses you
2. Weekly, review your anomalies
3. Look for patterns or connections
4. Research the most interesting anomalies
5. Share puzzling ones with others for perspective

## The Path Forward

Unknown unknowns are humbling. They remind us that our mental models, no matter how sophisticated, are always incomplete. The map is never the territory, and the territory itself keeps changing. But this humility is the beginning of wisdom.

In the next chapter, we'll explore a different kind of challenge: optimal stopping problems. When should you stop gathering information and commit to a decision? When should you stop exploring options and start exploiting what you've found? These questions involve a fundamental trade-off between learning and doing, between opportunity and commitment.

The unknown unknowns remind us that we never have complete information. Optimal stopping tells us when incomplete information is enough. Together, they frame the central challenge of decision-making: acting wisely despite irreducible ignorance.

---

¹ Rumsfeld, D. (2002). Department of Defense News Briefing. February 12, 2002. Washington, D.C.

² Taleb, N. N. (2007). The Black Swan: The Impact of the Highly Improbable. New York: Random House.

³ Kahneman, D. (2011). Thinking, Fast and Slow. New York: Farrar, Straus and Giroux, p. 86.

⁴ Klein, G. (2007). "Performing a project premortem." Harvard Business Review, 85(9), 18-19.

⁵ Immerman, R. H. (2011). "Transforming Analysis: The Intelligence Community's Best Kept Secret." Intelligence and National Security, 26(2-3), 159-181.

⁶ Martin, R. L., & Lafley, A. G. (2013). Playing to Win: How Strategy Really Works. Boston: Harvard Business Review Press.

⁷ Wack, P. (1985). "Scenarios: Shooting the rapids." Harvard Business Review, 63(6), 139-150.

⁸ Klein, G. (2013). Seeing What Others Don't: The Remarkable Ways We Gain Insights. New York: PublicAffairs.

⁹ Taleb, N. N. (2012). Antifragile: Things That Gain from Disorder. New York: Random House.

¹⁰ Taleb, N. N. (2012). Antifragile: Things That Gain from Disorder. New York: Random House, pp. 301-320.

¹¹ Feynman, R. P. (1988). What Do You Care What Other People Think? New York: W. W. Norton & Company.